{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author Hayden Gonne \n",
    "# Edit at june 10, 2024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VyOckJu6Rs-i"
   },
   "source": [
    "# 데이터 증강"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-datasets in c:\\users\\admin\\anaconda3\\envs\\face_recognition\\lib\\site-packages (4.5.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\admin\\anaconda3\\envs\\face_recognition\\lib\\site-packages (from tensorflow-datasets) (3.7.4.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\anaconda3\\envs\\face_recognition\\lib\\site-packages (from tensorflow-datasets) (4.64.1)\n",
      "Requirement already satisfied: dataclasses in c:\\users\\admin\\anaconda3\\envs\\face_recognition\\lib\\site-packages (from tensorflow-datasets) (0.8)\n",
      "Requirement already satisfied: tensorflow-metadata in c:\\users\\admin\\anaconda3\\envs\\face_recognition\\lib\\site-packages (from tensorflow-datasets) (1.2.0)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in c:\\users\\admin\\anaconda3\\envs\\face_recognition\\lib\\site-packages (from tensorflow-datasets) (3.17.2)\n",
      "Requirement already satisfied: promise in c:\\users\\admin\\anaconda3\\envs\\face_recognition\\lib\\site-packages (from tensorflow-datasets) (2.3)\n",
      "Requirement already satisfied: absl-py in c:\\users\\admin\\anaconda3\\envs\\face_recognition\\lib\\site-packages (from tensorflow-datasets) (0.12.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\admin\\anaconda3\\envs\\face_recognition\\lib\\site-packages (from tensorflow-datasets) (2.27.1)\n",
      "Requirement already satisfied: six in c:\\users\\admin\\anaconda3\\envs\\face_recognition\\lib\\site-packages (from tensorflow-datasets) (1.15.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\anaconda3\\envs\\face_recognition\\lib\\site-packages (from tensorflow-datasets) (1.19.2)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\admin\\anaconda3\\envs\\face_recognition\\lib\\site-packages (from tensorflow-datasets) (5.4.0)\n",
      "Requirement already satisfied: dill in c:\\users\\admin\\anaconda3\\envs\\face_recognition\\lib\\site-packages (from tensorflow-datasets) (0.3.4)\n",
      "Requirement already satisfied: termcolor in c:\\users\\admin\\anaconda3\\envs\\face_recognition\\lib\\site-packages (from tensorflow-datasets) (1.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\envs\\face_recognition\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\admin\\anaconda3\\envs\\face_recognition\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\anaconda3\\envs\\face_recognition\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\envs\\face_recognition\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (3.3)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\admin\\anaconda3\\envs\\face_recognition\\lib\\site-packages (from importlib-resources->tensorflow-datasets) (3.6.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in c:\\users\\admin\\anaconda3\\envs\\face_recognition\\lib\\site-packages (from tensorflow-metadata->tensorflow-datasets) (1.56.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\envs\\face_recognition\\lib\\site-packages (from tqdm->tensorflow-datasets) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow-datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-15T01:40:21.258663Z",
     "iopub.status.busy": "2022-12-15T01:40:21.258063Z",
     "iopub.status.idle": "2022-12-15T01:40:24.102677Z",
     "shell.execute_reply": "2022-12-15T01:40:24.101855Z"
    },
    "id": "C2Q5rPenTAJP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\envs\\Face_Recognition\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# 다음 변수를 수정하여 새로 만들 이미지 갯수를 정합니다.\n",
    "num_augmented_images = 30\n",
    "\n",
    "# 원본 폴더 경로\n",
    "file_path = r\"C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\train\\Hayden\"\n",
    "# 위의 폴더 내부에 있는 이미지 이름의 배열이 저장\n",
    "file_names = os.listdir(file_path)\n",
    "total_origin_image_num = len(file_names) # file_names의 길이 저장\n",
    "augment_cnt = 1 # 몇번의 augmentation이 일어났는지를 확인하기 위해 선언\n",
    "\n",
    "for i in range(1, num_augmented_images):\n",
    "    change_picture_index = random.randrange(1, total_origin_image_num) # 전체 이미지 개수 중 하나를 랜덤하게 선택하여 file_name에 그 인덱스를 갖는 이미지 이름을 저장\n",
    "    file_name = file_names[change_picture_index]\n",
    "    \n",
    "    origin_image_path = os.path.join(file_path, file_name)\n",
    "    image = Image.open(origin_image_path)\n",
    "    random_augment = random.randrange(1, 4) # 1~3의 무작위 수가 결정\n",
    "    \n",
    "    if random_augment == 1:\n",
    "        # 이미지 좌우 반전\n",
    "        inverted_image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        inverted_image.save(os.path.join(file_path, f'inverted_{augment_cnt}.png'))\n",
    "        \n",
    "    elif random_augment == 2:\n",
    "        # -20~20도로 이미지 기울이기\n",
    "        rotated_image = image.rotate(random.randrange(-20, 20))\n",
    "        rotated_image.save(os.path.join(file_path, f'rotated_{augment_cnt}.png'))\n",
    "        \n",
    "    elif random_augment == 3:\n",
    "        # 가우스 노이즈 추가하기\n",
    "        img = cv2.imread(origin_image_path)\n",
    "        row, col, ch = img.shape\n",
    "        mean = 0\n",
    "        var = 0.1\n",
    "        sigma = var**0.5\n",
    "        gauss = np.random.normal(mean, sigma, (row, col, ch))\n",
    "        gauss = gauss.reshape(row, col, ch)\n",
    "        noisy_array = img + gauss\n",
    "        noisy_image = Image.fromarray(np.uint8(noisy_array)).convert('RGB')\n",
    "        noisy_image.save(os.path.join(file_path, f'noiseAdded_{augment_cnt}.png'))\n",
    "        \n",
    "    augment_cnt += 1 # 한바퀴 돌 때마다 1씩 업데이트\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그레이 스케일 조정, 명암 조절, 히스토그램 조절, convert, 엣지 컬러 캐니 등 추가 - 엣지 컬러 캐니 제거 \n",
    "# second made by Hayden Gonne\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image, ImageEnhance, ImageOps\n",
    "\n",
    "# 새로 만들 이미지 갯수 설정\n",
    "num_augmented_images = 30\n",
    "\n",
    "# 원본 폴더 경로\n",
    "file_path = r\"C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\train\\Jaehyung\"\n",
    "# 이미지 파일 이름 배열\n",
    "file_names = os.listdir(file_path)\n",
    "total_origin_image_num = len(file_names)\n",
    "augment_cnt = 1\n",
    "\n",
    "for i in range(1, num_augmented_images):\n",
    "    change_picture_index = random.randrange(1, total_origin_image_num)\n",
    "    file_name = file_names[change_picture_index]\n",
    "    \n",
    "    origin_image_path = os.path.join(file_path, file_name)\n",
    "    image = Image.open(origin_image_path)\n",
    "    random_augment = random.randrange(1, 9)  # 1~8의 무작위 수 결정\n",
    "    \n",
    "    if random_augment == 1:\n",
    "        # 이미지 좌우 반전\n",
    "        inverted_image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        inverted_image.save(os.path.join(file_path, f'inverted_{augment_cnt}.png'))\n",
    "        \n",
    "    elif random_augment == 2:\n",
    "        # -20~20도로 이미지 기울이기\n",
    "        rotated_image = image.rotate(random.randrange(-20, 20))\n",
    "        rotated_image.save(os.path.join(file_path, f'rotated_{augment_cnt}.png'))\n",
    "        \n",
    "    elif random_augment == 3:\n",
    "        # 가우스 노이즈 추가하기\n",
    "        img = cv2.imread(origin_image_path)\n",
    "        row, col, ch = img.shape\n",
    "        mean = 0\n",
    "        var = 0.1\n",
    "        sigma = var**0.5\n",
    "        gauss = np.random.normal(mean, sigma, (row, col, ch))\n",
    "        noisy_array = img + gauss\n",
    "        noisy_image = Image.fromarray(np.uint8(noisy_array)).convert('RGB')\n",
    "        noisy_image.save(os.path.join(file_path, f'noiseAdded_{augment_cnt}.png'))\n",
    "        \n",
    "    elif random_augment == 4:\n",
    "        # 그레이스케일 변환\n",
    "        grayscale_image = ImageOps.grayscale(image)\n",
    "        grayscale_image.save(os.path.join(file_path, f'grayscale_{augment_cnt}.png'))\n",
    "        \n",
    "    elif random_augment == 5:\n",
    "        # 명암 조절\n",
    "        enhancer = ImageEnhance.Contrast(image)\n",
    "        contrasted_image = enhancer.enhance(random.uniform(0.5, 1.5))\n",
    "        contrasted_image.save(os.path.join(file_path, f'contrasted_{augment_cnt}.png'))\n",
    "        \n",
    "    elif random_augment == 6:\n",
    "        # 히스토그램 평활화\n",
    "        img = cv2.imread(origin_image_path)\n",
    "        img_yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)\n",
    "        img_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0])\n",
    "        hist_equalized_image = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)\n",
    "        hist_equalized_image = Image.fromarray(hist_equalized_image)\n",
    "        hist_equalized_image.save(os.path.join(file_path, f'hist_equalized_{augment_cnt}.png'))\n",
    "        \n",
    "    elif random_augment == 7:\n",
    "        # 색상 변환\n",
    "        converted_image = image.convert('L').convert('RGB')\n",
    "        converted_image.save(os.path.join(file_path, f'converted_{augment_cnt}.png'))\n",
    "        \n",
    "    elif random_augment == 8:\n",
    "        # 엣지 검출 후 컬러\n",
    "        img = cv2.imread(origin_image_path)\n",
    "        edges = cv2.Canny(img, 100, 200)\n",
    "        edges_colored = cv2.bitwise_and(img, img, mask=edges)\n",
    "        edges_colored_image = Image.fromarray(edges_colored)\n",
    "        edges_colored_image.save(os.path.join(file_path, f'edges_colored_{augment_cnt}.png'))\n",
    "        \n",
    "    augment_cnt += 1  # 한 바퀴 돌 때마다 1씩 업데이트\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images: 51, Test images: 13\n"
     ]
    }
   ],
   "source": [
    "# 인풋 데이터 넣으면 전처리→ 증강 → 생성된 이미지들로 train, test가 자동으로 분류되어 각 폴더에 저장되는 파이프 라인\n",
    "# second made by Hayden Gonne\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image, ImageEnhance, ImageOps\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 전처리 레이어 설정\n",
    "IMG_SIZE = 180\n",
    "\n",
    "preprocess_layers = tf.keras.Sequential([\n",
    "    tf.keras.layers.Resizing(IMG_SIZE, IMG_SIZE),\n",
    "    tf.keras.layers.Rescaling(1./255)\n",
    "])\n",
    "\n",
    "# 증강 이미지 수 설정\n",
    "num_augmented_images = 70\n",
    "\n",
    "# 원본 폴더 경로 설정\n",
    "file_path = r\"C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June\"\n",
    "output_path = r\"C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\"\n",
    "\n",
    "# 이미지 파일 배열\n",
    "file_names = os.listdir(file_path)\n",
    "total_origin_image_num = len(file_names)\n",
    "augment_cnt = 1\n",
    "\n",
    "# 데이터 증강 함수\n",
    "def augment_images(file_path, output_path, num_augmented_images):\n",
    "    file_names = os.listdir(file_path)\n",
    "    total_origin_image_num = len(file_names)\n",
    "    augment_cnt = 1\n",
    "\n",
    "    for i in range(1, num_augmented_images):\n",
    "        change_picture_index = random.randrange(1, total_origin_image_num)\n",
    "        file_name = file_names[change_picture_index]\n",
    "        \n",
    "        origin_image_path = os.path.join(file_path, file_name)\n",
    "        image = Image.open(origin_image_path)\n",
    "        random_augment = random.randrange(1, 9)\n",
    "        \n",
    "        if random_augment == 1:\n",
    "            # 이미지 좌우 반전\n",
    "            inverted_image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            inverted_image.save(os.path.join(file_path, f'inverted_{augment_cnt}.png'))\n",
    "            \n",
    "        elif random_augment == 2:\n",
    "            # -20~20도로 이미지 기울이기\n",
    "            rotated_image = image.rotate(random.randrange(-20, 20))\n",
    "            rotated_image.save(os.path.join(file_path, f'rotated_{augment_cnt}.png'))\n",
    "            \n",
    "        elif random_augment == 3:\n",
    "            # 가우스 노이즈 추가하기\n",
    "            img = cv2.imread(origin_image_path)\n",
    "            row, col, ch = img.shape\n",
    "            mean = 0\n",
    "            var = 0.1\n",
    "            sigma = var**0.5\n",
    "            gauss = np.random.normal(mean, sigma, (row, col, ch))\n",
    "            noisy_array = img + gauss\n",
    "            noisy_image = Image.fromarray(np.uint8(noisy_array)).convert('RGB')\n",
    "            noisy_image.save(os.path.join(file_path, f'noiseAdded_{augment_cnt}.png'))\n",
    "            \n",
    "        elif random_augment == 4:\n",
    "            # 그레이스케일 변환\n",
    "            grayscale_image = ImageOps.grayscale(image)\n",
    "            grayscale_image.save(os.path.join(file_path, f'grayscale_{augment_cnt}.png'))\n",
    "            \n",
    "        elif random_augment == 5:\n",
    "            # 명암 조절\n",
    "            enhancer = ImageEnhance.Contrast(image)\n",
    "            contrasted_image = enhancer.enhance(random.uniform(0.5, 1.5))\n",
    "            contrasted_image.save(os.path.join(file_path, f'contrasted_{augment_cnt}.png'))\n",
    "            \n",
    "        elif random_augment == 6:\n",
    "            # 히스토그램 평활화\n",
    "            img = cv2.imread(origin_image_path)\n",
    "            img_yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)\n",
    "            img_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0])\n",
    "            hist_equalized_image = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)\n",
    "            hist_equalized_image = Image.fromarray(hist_equalized_image)\n",
    "            hist_equalized_image.save(os.path.join(file_path, f'hist_equalized_{augment_cnt}.png'))\n",
    "            \n",
    "        elif random_augment == 7:\n",
    "            # 색상 변환\n",
    "            converted_image = image.convert('L').convert('RGB')\n",
    "            converted_image.save(os.path.join(file_path, f'converted_{augment_cnt}.png'))\n",
    "            \n",
    "        augment_cnt += 1  # 한 바퀴 돌 때마다 1씩 업데이트\n",
    "\n",
    "# 전처리 및 증강 실행\n",
    "augment_images(file_path, output_path, num_augmented_images)\n",
    "\n",
    "# 이미지 파일 경로 목록 생성\n",
    "all_files = [os.path.join(file_path, f) for f in os.listdir(file_path) if f.endswith(('jpg', 'png'))]\n",
    "\n",
    "# 학습 및 테스트 세트로 분할\n",
    "train_files, test_files = train_test_split(all_files, test_size=0.2, random_state=42)\n",
    "\n",
    "# 폴더 생성\n",
    "train_dir = os.path.join(output_path, 'June_train')\n",
    "test_dir = os.path.join(output_path, 'June_test')\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# 파일 복사\n",
    "for file in train_files:\n",
    "    shutil.copy(file, train_dir)\n",
    "for file in test_files:\n",
    "    shutil.copy(file, test_dir)\n",
    "\n",
    "print(f'Train images: {len(train_files)}, Test images: {len(test_files)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images: 99, Test images: 25\n"
     ]
    }
   ],
   "source": [
    "# 인풋 데이터 넣으면 전처리→ 증강 → 생성된 이미지들로 train, test가 자동으로 분류되어 각 폴더에 저장되는 파이프 라인\n",
    "# second made by Hayden Gonne\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image, ImageEnhance, ImageOps\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 전처리 레이어 설정\n",
    "IMG_SIZE = 180\n",
    "\n",
    "preprocess_layers = tf.keras.Sequential([\n",
    "    tf.keras.layers.Resizing(IMG_SIZE, IMG_SIZE),\n",
    "    tf.keras.layers.Rescaling(1./255)\n",
    "])\n",
    "\n",
    "# 증강 이미지 수 설정\n",
    "num_augmented_images = 70\n",
    "\n",
    "# 원본 폴더 경로 설정\n",
    "file_path = r\"C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Youngrok\"\n",
    "output_path = r\"C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\"\n",
    "\n",
    "# 이미지 파일 배열\n",
    "file_names = os.listdir(file_path)\n",
    "total_origin_image_num = len(file_names)\n",
    "augment_cnt = 1\n",
    "\n",
    "# 데이터 증강 함수\n",
    "def augment_images(file_path, output_path, num_augmented_images):\n",
    "    file_names = os.listdir(file_path)\n",
    "    total_origin_image_num = len(file_names)\n",
    "    augment_cnt = 1\n",
    "\n",
    "    for i in range(1, num_augmented_images):\n",
    "        change_picture_index = random.randrange(1, total_origin_image_num)\n",
    "        file_name = file_names[change_picture_index]\n",
    "        \n",
    "        origin_image_path = os.path.join(file_path, file_name)\n",
    "        image = Image.open(origin_image_path)\n",
    "        random_augment = random.randrange(1, 9)\n",
    "        \n",
    "        if random_augment == 1:\n",
    "            # 이미지 좌우 반전\n",
    "            inverted_image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            inverted_image.save(os.path.join(file_path, f'inverted_{augment_cnt}.png'))\n",
    "            \n",
    "        elif random_augment == 2:\n",
    "            # -20~20도로 이미지 기울이기\n",
    "            rotated_image = image.rotate(random.randrange(-20, 20))\n",
    "            rotated_image.save(os.path.join(file_path, f'rotated_{augment_cnt}.png'))\n",
    "            \n",
    "        elif random_augment == 3:\n",
    "            # 가우스 노이즈 추가하기\n",
    "            img = cv2.imread(origin_image_path)\n",
    "            row, col, ch = img.shape\n",
    "            mean = 0\n",
    "            var = 0.1\n",
    "            sigma = var**0.5\n",
    "            gauss = np.random.normal(mean, sigma, (row, col, ch))\n",
    "            noisy_array = img + gauss\n",
    "            noisy_image = Image.fromarray(np.uint8(noisy_array)).convert('RGB')\n",
    "            noisy_image.save(os.path.join(file_path, f'noiseAdded_{augment_cnt}.png'))\n",
    "            \n",
    "        elif random_augment == 4:\n",
    "            # 그레이스케일 변환\n",
    "            grayscale_image = ImageOps.grayscale(image)\n",
    "            grayscale_image.save(os.path.join(file_path, f'grayscale_{augment_cnt}.png'))\n",
    "            \n",
    "        elif random_augment == 5:\n",
    "            # 명암 조절\n",
    "            enhancer = ImageEnhance.Contrast(image)\n",
    "            contrasted_image = enhancer.enhance(random.uniform(0.5, 1.5))\n",
    "            contrasted_image.save(os.path.join(file_path, f'contrasted_{augment_cnt}.png'))\n",
    "            \n",
    "        elif random_augment == 6:\n",
    "            # 히스토그램 평활화\n",
    "            img = cv2.imread(origin_image_path)\n",
    "            img_yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)\n",
    "            img_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0])\n",
    "            hist_equalized_image = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)\n",
    "            hist_equalized_image = Image.fromarray(hist_equalized_image)\n",
    "            hist_equalized_image.save(os.path.join(file_path, f'hist_equalized_{augment_cnt}.png'))\n",
    "            \n",
    "        elif random_augment == 7:\n",
    "            # 색상 변환\n",
    "            converted_image = image.convert('L').convert('RGB')\n",
    "            converted_image.save(os.path.join(file_path, f'converted_{augment_cnt}.png'))\n",
    "            \n",
    "        augment_cnt += 1  # 한 바퀴 돌 때마다 1씩 업데이트\n",
    "\n",
    "# 전처리 및 증강 실행\n",
    "augment_images(file_path, output_path, num_augmented_images)\n",
    "\n",
    "# 이미지 파일 경로 목록 생성\n",
    "all_files = [os.path.join(file_path, f) for f in os.listdir(file_path) if f.endswith(('jpg', 'png'))]\n",
    "\n",
    "# 학습 및 테스트 세트로 분할\n",
    "train_files, test_files = train_test_split(all_files, test_size=0.2, random_state=42)\n",
    "\n",
    "# 폴더 생성\n",
    "train_dir = os.path.join(output_path, 'Youngrok_train')\n",
    "test_dir = os.path.join(output_path, 'Youngrok_test')\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# 파일 복사\n",
    "for file in train_files:\n",
    "    shutil.copy(file, train_dir)\n",
    "for file in test_files:\n",
    "    shutil.copy(file, test_dir)\n",
    "\n",
    "print(f'Train images: {len(train_files)}, Test images: {len(test_files)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After data augmentation, redo image classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Hayden_train\\B.jpg\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Hayden_train\\contrasted_16.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Hayden_train\\hist_equalized_18.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Hayden_train\\hist_equalized_5.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Hayden_train\\noiseAdded_27.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Hayden_train\\rotated_13.png\n",
      ">loaded 122 examples for class: Hayden\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Jaehyung_train\\converted_31.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Jaehyung_train\\converted_49.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Jaehyung_train\\converted_61.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Jaehyung_train\\edges_colored_11.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Jaehyung_train\\edges_colored_18.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Jaehyung_train\\edges_colored_22.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Jaehyung_train\\edges_colored_26.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Jaehyung_train\\grayscale_10.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Jaehyung_train\\hist_equalized_20.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Jaehyung_train\\hist_equalized_26.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Jaehyung_train\\hist_equalized_29.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Jaehyung_train\\hist_equalized_6.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Jaehyung_train\\hist_equalized_9.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Jaehyung_train\\inverted_25.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Jaehyung_train\\noiseAdded_27.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Jaehyung_train\\noiseAdded_71.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Jaehyung_train\\rotated_27.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Jaehyung_train\\rotated_40.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Jaehyung_train\\rotated_42.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Jaehyung_train\\rotated_8.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Jaehyung_train\\rotated_9.png\n",
      ">loaded 109 examples for class: Jaehyung\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_train\\contrasted_29.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_train\\contrasted_3.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_train\\contrasted_39.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_train\\contrasted_46.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_train\\contrasted_51.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_train\\contrasted_60.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_train\\contrasted_61.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_train\\converted_21.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_train\\converted_22.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_train\\converted_31.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_train\\converted_32.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_train\\converted_48.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_train\\converted_55.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_train\\grayscale_1.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_train\\grayscale_11.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_train\\grayscale_12.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_train\\grayscale_23.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_train\\grayscale_27.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_train\\grayscale_33.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_train\\hist_equalized_25.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_train\\hist_equalized_28.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_train\\hist_equalized_5.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_train\\hist_equalized_53.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_train\\hist_equalized_8.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_train\\inverted_10.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_train\\inverted_17.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_train\\inverted_4.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_train\\inverted_45.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_train\\inverted_54.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_train\\inverted_57.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_train\\inverted_69.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_train\\KakaoTalk_20240611_205242745.jpg\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_train\\KakaoTalk_20240611_205242745_01.jpg\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_train\\KakaoTalk_20240611_205242745_03.jpg\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_train\\KakaoTalk_20240611_205242745_04.jpg\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_train\\noiseAdded_24.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_train\\noiseAdded_41.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_train\\rotated_36.png\n",
      ">loaded 13 examples for class: June\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Sehun_train\\converted_24.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Sehun_train\\converted_54.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Sehun_train\\grayscale_12.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Sehun_train\\grayscale_14.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Sehun_train\\grayscale_31.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Sehun_train\\grayscale_4.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Sehun_train\\grayscale_45.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Sehun_train\\grayscale_61.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Sehun_train\\grayscale_67.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Sehun_train\\hist_equalized_15.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Sehun_train\\hist_equalized_19.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Sehun_train\\hist_equalized_21.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Sehun_train\\hist_equalized_9.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Sehun_train\\inverted_38.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Sehun_train\\noiseAdded_10.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Sehun_train\\noiseAdded_11.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Sehun_train\\noiseAdded_2.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Sehun_train\\noiseAdded_27.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Sehun_train\\noiseAdded_35.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Sehun_train\\noiseAdded_48.png\n",
      ">loaded 67 examples for class: Sehun\n",
      ">loaded 99 examples for class: Youngrok\n",
      "(410, 160, 160, 3) (410,)\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Hayden_test\\B.jpg\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Hayden_test\\hist_equalized_5.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Hayden_test\\noiseAdded_27.png\n",
      ">loaded 41 examples for class: Hayden\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Jaehyung_test\\converted_31.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Jaehyung_test\\edges_colored_11.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Jaehyung_test\\hist_equalized_29.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Jaehyung_test\\rotated_8.png\n",
      ">loaded 44 examples for class: Jaehyung\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_test\\contrasted_18.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_test\\contrasted_43.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_test\\converted_26.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_test\\grayscale_14.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_test\\inverted_56.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_test\\noiseAdded_2.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_test\\noiseAdded_42.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\June_test\\noiseAdded_7.png\n",
      ">loaded 5 examples for class: June\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Sehun_test\\converted_11.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Sehun_test\\hist_equalized_32.png\n",
      "No face detected in C:\\Users\\admin\\Desktop\\face_recognition_project\\dataset\\Sehun_test\\hist_equalized_5.png\n",
      ">loaded 19 examples for class: Sehun\n",
      ">loaded 25 examples for class: Youngrok\n",
      "(134, 160, 160, 3) (134,)\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isdir, join\n",
    "from PIL import Image\n",
    "from numpy import savez_compressed, asarray\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "\n",
    "# MTCNN 탐지기를 전역으로 생성\n",
    "detector = MTCNN()\n",
    "\n",
    "# 주어진 사진에서 단일 얼굴을 추출하는 함수\n",
    "def extract_face(filename, required_size=(160, 160)):\n",
    "    try:\n",
    "        # 파일에서 이미지 로드\n",
    "        image = Image.open(filename)\n",
    "        # 필요시 RGB로 변환 (이미지가 다른 모드일 수 있으므로)\n",
    "        image = image.convert('RGB')\n",
    "        # 이미지를 배열로 변환\n",
    "        pixels = asarray(image)\n",
    "        # 이미지에서 얼굴 감지\n",
    "        results = detector.detect_faces(pixels)\n",
    "        if len(results) == 0:\n",
    "            print(f\"No face detected in {filename}\")\n",
    "            return None\n",
    "        # 첫 번째 얼굴에서 바운딩 박스 추출\n",
    "        x1, y1, width, height = results[0]['box']\n",
    "        # 바운딩 박스가 음수일 경우를 위한 오류 수정\n",
    "        x1, y1 = abs(x1), abs(y1)\n",
    "        x2, y2 = x1 + width, y1 + height\n",
    "        # 얼굴 추출\n",
    "        face = pixels[y1:y2, x1:x2]\n",
    "        # 모델 크기에 맞게 픽셀 크기 조정\n",
    "        image = Image.fromarray(face)\n",
    "        image = image.resize(required_size)\n",
    "        face_array = asarray(image)\n",
    "        return face_array\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image: {filename}, error: {e}\")\n",
    "        return None\n",
    "\n",
    "# 디렉토리 내 모든 이미지 로드 및 얼굴 추출 함수\n",
    "def load_faces(directory, required_size=(160, 160)):\n",
    "    faces = list()\n",
    "    # 파일 나열\n",
    "    for filename in listdir(directory):\n",
    "        # 경로 생성\n",
    "        path = join(directory, filename)\n",
    "        # 얼굴 추출\n",
    "        face = extract_face(path)\n",
    "        if face is not None:\n",
    "            # 얼굴 저장\n",
    "            faces.append(face)\n",
    "    return faces\n",
    "\n",
    "# 데이터셋 로드 함수 (클래스별 하위 디렉토리를 포함)\n",
    "def load_dataset(directory, subset, required_size=(160, 160)):\n",
    "    X, y = list(), list()\n",
    "    # 클래스별 폴더 나열\n",
    "    for subdir in listdir(directory):\n",
    "        # 디렉토리가 아닌 파일은 무시\n",
    "        if '_train' in subdir or '_test' in subdir:\n",
    "            continue\n",
    "        # 하위 디렉토리 내 모든 얼굴 로드\n",
    "        person_train_path = join(directory, subdir + '_train')\n",
    "        person_test_path = join(directory, subdir + '_test')\n",
    "        if subset == 'train':\n",
    "            faces = load_faces(person_train_path)\n",
    "        else:\n",
    "            faces = load_faces(person_test_path)\n",
    "        # 라벨 생성\n",
    "        labels = [subdir for _ in range(len(faces))]\n",
    "        # 진행 상황 요약 출력\n",
    "        print('>loaded %d examples for class: %s' % (len(faces), subdir))\n",
    "        # 데이터와 라벨 저장\n",
    "        X.extend(faces)\n",
    "        y.extend(labels)\n",
    "    return asarray(X), asarray(y)\n",
    "\n",
    "# 훈련 데이터셋 로드\n",
    "trainX, trainy = load_dataset('C:\\\\Users\\\\admin\\\\Desktop\\\\face_recognition_project\\\\dataset', 'train')\n",
    "print(trainX.shape, trainy.shape)\n",
    "\n",
    "# 테스트 데이터셋 로드\n",
    "testX, testy = load_dataset('C:\\\\Users\\\\admin\\\\Desktop\\\\face_recognition_project\\\\dataset', 'test')\n",
    "print(testX.shape, testy.shape)\n",
    "\n",
    "# 압축 형식으로 배열을 하나의 파일에 저장\n",
    "savez_compressed('C:\\\\Users\\\\admin\\\\Desktop\\\\face_recognition_project\\\\face_recognition_project.npz', trainX, trainy, testX, testy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded:  (123, 128) (123,) (16, 128) (16,)\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    c:\\Users\\admin\\anaconda3\\envs\\Face_Recognition\\lib\\site-packages\\keras\\engine\\training.py:1586 predict_function  *\n        return step_function(self, iterator)\n    c:\\Users\\admin\\anaconda3\\envs\\Face_Recognition\\lib\\site-packages\\keras\\engine\\training.py:1576 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    c:\\Users\\admin\\anaconda3\\envs\\Face_Recognition\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    c:\\Users\\admin\\anaconda3\\envs\\Face_Recognition\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    c:\\Users\\admin\\anaconda3\\envs\\Face_Recognition\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    c:\\Users\\admin\\anaconda3\\envs\\Face_Recognition\\lib\\site-packages\\keras\\engine\\training.py:1569 run_step  **\n        outputs = model.predict_step(data)\n    c:\\Users\\admin\\anaconda3\\envs\\Face_Recognition\\lib\\site-packages\\keras\\engine\\training.py:1537 predict_step\n        return self(x, training=False)\n    c:\\Users\\admin\\anaconda3\\envs\\Face_Recognition\\lib\\site-packages\\keras\\engine\\base_layer.py:1020 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    c:\\Users\\admin\\anaconda3\\envs\\Face_Recognition\\lib\\site-packages\\keras\\engine\\input_spec.py:269 assert_input_compatibility\n        ', found shape=' + display_shape(x.shape))\n\n    ValueError: Input 0 is incompatible with layer inception_resnet_v1: expected shape=(None, 160, 160, 3), found shape=(None, 128)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-b12cfd9ed70d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mnewTrainX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mface_pixels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0membedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mface_pixels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0mnewTrainX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0mnewTrainX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewTrainX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-b12cfd9ed70d>\u001b[0m in \u001b[0;36mget_embedding\u001b[1;34m(model, face_pixels)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mface_pixels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# make prediction to get embedding 모델을 사용하여 임베딩을 예측하고 결과 벡터를 반환\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0myhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0myhat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\Face_Recognition\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1749\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1750\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1751\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1752\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1753\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\Face_Recognition\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\Face_Recognition\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\Face_Recognition\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    758\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m    759\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[1;32m--> 760\u001b[1;33m             *args, **kwds))\n\u001b[0m\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\Face_Recognition\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3064\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3065\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3066\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3067\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\Face_Recognition\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3463\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\Face_Recognition\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3306\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3307\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3308\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   3309\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3310\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\Face_Recognition\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[0;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1007\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1008\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1009\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\Face_Recognition\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    666\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 668\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    669\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\Face_Recognition\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    992\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    993\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 994\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    995\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    c:\\Users\\admin\\anaconda3\\envs\\Face_Recognition\\lib\\site-packages\\keras\\engine\\training.py:1586 predict_function  *\n        return step_function(self, iterator)\n    c:\\Users\\admin\\anaconda3\\envs\\Face_Recognition\\lib\\site-packages\\keras\\engine\\training.py:1576 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    c:\\Users\\admin\\anaconda3\\envs\\Face_Recognition\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    c:\\Users\\admin\\anaconda3\\envs\\Face_Recognition\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    c:\\Users\\admin\\anaconda3\\envs\\Face_Recognition\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    c:\\Users\\admin\\anaconda3\\envs\\Face_Recognition\\lib\\site-packages\\keras\\engine\\training.py:1569 run_step  **\n        outputs = model.predict_step(data)\n    c:\\Users\\admin\\anaconda3\\envs\\Face_Recognition\\lib\\site-packages\\keras\\engine\\training.py:1537 predict_step\n        return self(x, training=False)\n    c:\\Users\\admin\\anaconda3\\envs\\Face_Recognition\\lib\\site-packages\\keras\\engine\\base_layer.py:1020 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    c:\\Users\\admin\\anaconda3\\envs\\Face_Recognition\\lib\\site-packages\\keras\\engine\\input_spec.py:269 assert_input_compatibility\n        ', found shape=' + display_shape(x.shape))\n\n    ValueError: Input 0 is incompatible with layer inception_resnet_v1: expected shape=(None, 160, 160, 3), found shape=(None, 128)\n"
     ]
    }
   ],
   "source": [
    "# calculate a face embedding for each face in the dataset using facenet\n",
    "from numpy import load\n",
    "from numpy import expand_dims\n",
    "from numpy import asarray\n",
    "from numpy import savez_compressed\n",
    "from keras.models import load_model\n",
    "\n",
    "# get the face embedding for one face 얼굴 임베딩을 계산하는 함수 정의 : 주어진 얼굴 이미지의 픽셀 데이터를 FaceNet 모델을 통해 임베딩 벡터로 변환\n",
    "def get_embedding(model, face_pixels):\n",
    "\t# scale pixel values -> float32\n",
    "\tface_pixels = face_pixels.astype('float32')\n",
    "\t# standardize pixel values across channels (global) 전체 이미지의 평균과 표준 편차를 사용하여 픽셀 값을 표준화\n",
    "\tmean, std = face_pixels.mean(), face_pixels.std()\n",
    "\tface_pixels = (face_pixels - mean) / std\n",
    "\t# transform face into one sample 모델이 예측할 수 있도록 샘플의 차원을 확장\n",
    "\tsamples = expand_dims(face_pixels, axis=0)\n",
    "\t# make prediction to get embedding 모델을 사용하여 임베딩을 예측하고 결과 벡터를 반환\n",
    "\tyhat = model.predict(samples)\n",
    "\treturn yhat[0]\n",
    "\n",
    "# load the face dataset\n",
    "data = load('face_recognition_project.npz')\n",
    "trainX, trainy, testX, testy = data['arr_0'], data['arr_1'], data['arr_2'], data['arr_3']\n",
    "print('Loaded: ', trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
    "\n",
    "# load the facenet model\n",
    "model_path = 'C:\\\\Users\\\\admin\\Desktop\\\\face_recognition_project\\\\models\\\\facenet_keras.h5'\n",
    "model = load_model(model_path)\n",
    "\n",
    "# convert each face in the train set to an embedding 훈련 데이터셋의 얼굴 임베딩 계산\n",
    "# 훈련 데이터셋의 얼굴 이미지를 FaceNet을 사용해 임베딩 벡터로 변환하고 newTrainX 리스트에 추가, 배열로 변환\n",
    "newTrainX = list()\n",
    "for face_pixels in trainX:\n",
    "\tembedding = get_embedding(model, face_pixels)\n",
    "\tnewTrainX.append(embedding)\n",
    "newTrainX = asarray(newTrainX)\n",
    "print(newTrainX.shape)\n",
    "\n",
    "# convert each face in the test set to an embedding 테스트 데이터셋에도 똑같이 적용\n",
    "newTestX = list()\n",
    "for face_pixels in testX:\n",
    "\tembedding = get_embedding(model, face_pixels)\n",
    "\tnewTestX.append(embedding)\n",
    "newTestX = asarray(newTestX)\n",
    "print(newTestX.shape)\n",
    "\n",
    "# save arrays to one file in compressed format 계산된 임베딩 벡터와 레이블을 압축된 npz 파일로 저장해 나중에 쉽게 로드하여 사용할 수 있게 함.\n",
    "savez_compressed('face_recognition_project.npz', newTrainX, trainy, newTestX, testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the images\n",
    "trainX = trainX.reshape((trainX.shape[0], -1))\n",
    "testX = testX.reshape((testX.shape[0], -1))\n",
    "\n",
    "# Normalize input vectors\n",
    "in_encoder = Normalizer(norm='l2')\n",
    "trainX = in_encoder.transform(trainX)\n",
    "testX = in_encoder.transform(testX)\n",
    "\n",
    "print('Dataset: train=%d, test=%d' % (trainX.shape[0], testX.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  scikit-learn의 LabelEncoder 클래스를 통해 각 이름에 대한 문자열 대상 변수를 정수로 변환해야 함.\n",
    "# label encode targets \n",
    "# 문자열 형태의 클래스 레이블이 정수로 변환\n",
    "# LabelEncoder는 fit 메서드를 사용하여 클래스 레이블을 학습하고, transform 메서드를 사용하여 실제 변환을 수행\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create a LabelEncoder instance\n",
    "out_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder to the training labels\n",
    "out_encoder.fit(trainy)\n",
    "\n",
    "# Transform the training and testing labels\n",
    "trainy = out_encoder.transform(trainy)\n",
    "testy = out_encoder.transform(testy)\n",
    "\n",
    "print('Dataset: train=%d, test=%d' % (trainX.shape[0], testX.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create an SVC model with a linear kernel\n",
    "model = SVC(kernel='linear')\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(trainX, trainy)\n",
    "\n",
    "print('Model training completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: Youngrok (94.086)\n",
      "Expected: Youngrok\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (128,) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-dd7d0fa0cdee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;31m# plot for fun 예측된 이름 및 확률과 함께 얼굴 픽셀을 그린다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m \u001b[0mpyplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_face_pixels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'%s (%.3f)'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpredict_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_probability\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\Face_Recognition\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[1;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[0;32m   2728\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2729\u001b[0m         \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"data\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2730\u001b[1;33m         **kwargs)\n\u001b[0m\u001b[0;32m   2731\u001b[0m     \u001b[0msci\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2732\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\Face_Recognition\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1446\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1447\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[0mbound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\Face_Recognition\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5521\u001b[0m                               resample=resample, **kwargs)\n\u001b[0;32m   5522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5523\u001b[1;33m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5524\u001b[0m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5525\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\Face_Recognition\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36mset_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    710\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[0;32m    711\u001b[0m             raise TypeError(\"Invalid shape {} for image data\"\n\u001b[1;32m--> 712\u001b[1;33m                             .format(self._A.shape))\n\u001b[0m\u001b[0;32m    713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid shape (128,) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# develop a classifier for the Faces Dataset\n",
    "from random import choice\n",
    "from numpy import load\n",
    "from numpy import expand_dims\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.svm import SVC\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# load faces\n",
    "data = load('face_recognition_project.npz')\n",
    "testX_faces = data['arr_2']\n",
    "\n",
    "# load face embeddings\n",
    "data = load('face_recognition_project.npz')\n",
    "trainX, trainy, testX, testy = data['arr_0'], data['arr_1'], data['arr_2'], data['arr_3']\n",
    "\n",
    "# Flatten the images\n",
    "trainX = trainX.reshape((trainX.shape[0], -1))\n",
    "testX = testX.reshape((testX.shape[0], -1)) \n",
    "\n",
    "# normalize input vectors\n",
    "in_encoder = Normalizer(norm='l2')\n",
    "trainX = in_encoder.transform(trainX)\n",
    "testX = in_encoder.transform(testX)\n",
    "\n",
    "# label encode targets\n",
    "out_encoder = LabelEncoder()\n",
    "out_encoder.fit(trainy)\n",
    "trainy = out_encoder.transform(trainy)\n",
    "testy = out_encoder.transform(testy)\n",
    "\n",
    "# fit model\n",
    "model = SVC(kernel='linear', probability=True)\n",
    "model.fit(trainX, trainy)\n",
    "\n",
    "# test model on a random example from the test dataset\n",
    "# 테스트 세트에서 임의의 예를 선택한 다음 임베딩, 얼굴 픽셀, 예상 클래스 예측 및 해당 클래스 이름을 가져옴\n",
    "selection = choice([i for i in range(testX.shape[0])])\n",
    "random_face_pixels = testX_faces[selection]\n",
    "random_face_emb = testX[selection]\n",
    "random_face_class = testy[selection]\n",
    "random_face_name = out_encoder.inverse_transform([random_face_class])\n",
    "\n",
    "# prediction for the face 클래스 정수와 예측 확률\n",
    "samples = expand_dims(random_face_emb, axis=0)\n",
    "yhat_class = model.predict(samples)\n",
    "yhat_prob = model.predict_proba(samples)\n",
    "\n",
    "# get name 예측된 클래스 정수의 이름과 이 예측의 확률을 얻을 수 있다.\n",
    "class_index = yhat_class[0]\n",
    "class_probability = yhat_prob[0,class_index] * 100\n",
    "predict_names = out_encoder.inverse_transform(yhat_class)\n",
    "print('Predicted: %s (%.3f)' % (predict_names[0], class_probability))\n",
    "print('Expected: %s' % random_face_name[0])\n",
    "\n",
    "# plot for fun 예측된 이름 및 확률과 함께 얼굴 픽셀을 그린다.\n",
    "pyplot.imshow(random_face_pixels)\n",
    "title = '%s (%.3f)' % (predict_names[0], class_probability)\n",
    "pyplot.title(title)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: train=100.000, test=100.000\n"
     ]
    }
   ],
   "source": [
    "# 적합 모델을 사용하여 학습 및 테스트 데이터 세트의 각 예에 대한 예측을 수행한 다음 분류 정확도를 계산\n",
    "from sklearn.metrics import accuracy_score\n",
    "# predict \n",
    "yhat_train = model.predict(trainX)\n",
    "yhat_test = model.predict(testX)\n",
    "\n",
    "# score\n",
    "score_train = accuracy_score(trainy, yhat_train)\n",
    "score_test = accuracy_score(testy, yhat_test)\n",
    "\n",
    "# summarize\n",
    "print('Accuracy: train=%.3f, test=%.3f' % (score_train*100, score_test*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: train=93.325, test=91.746\n"
     ]
    }
   ],
   "source": [
    "# 적합 모델을 사용하여 학습 및 테스트 데이터 세트의 각 예에 대한 예측을 수행한 다음 분류 정확도를 계산\n",
    "from sklearn.metrics import accuracy_score\n",
    "# predict \n",
    "yhat_train = model.predict(trainX)\n",
    "yhat_test = model.predict(testX)\n",
    "\n",
    "# score\n",
    "score_train = accuracy_score(trainy, yhat_train)\n",
    "score_test = accuracy_score(testy, yhat_test)\n",
    "\n",
    "# summarize\n",
    "print('Accuracy: train=%.3f, test=%.3f' % (score_train*100, score_test*100))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "data_augmentation.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
